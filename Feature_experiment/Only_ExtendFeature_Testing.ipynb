{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Only_ExtendFeature_Testing.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1t6xMyF7nw-mt3DYr-m6TN4sVmrtdacXa","authorship_tag":"ABX9TyP3RnmQdb1LkicKRYldqCML"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOeMlMGP_ahl","executionInfo":{"status":"ok","timestamp":1628231852101,"user_tz":-60,"elapsed":213961,"user":{"displayName":"Pasinpat Vitoochuleechoti","photoUrl":"","userId":"04423538674438574487"}},"outputId":"cd7a9799-5606-4895-c8df-65bbe9c6f989"},"source":["import pandas as pd\n","import numpy as np\n","from IPython.display import display\n","from fastai.imports import *\n","from sklearn import metrics\n","from pandas.tseries.offsets import DateOffset\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","\n","\n","\n","hour_ahead = 24\n","path = \"/content/drive/MyDrive/Colab Notebooks/Project/Extend_FI.csv\"\n","Merge_all = pd.read_csv(path)\n","dti = pd.to_datetime(Merge_all['DateTime'], format='%m/%d/%Y %H', exact=False)\n","Merge_all.set_index(pd.Index(dti), inplace=True)\n","Merge_all.drop(['DateTime'], axis=1, inplace=True)\n","First = Merge_all.iloc[0:11000, :]\n","N_index = First.shape\n","Y_temp = First['FI']\n","Y = Y_temp.iloc[hour_ahead:N_index[0]]\n","Y = Y.to_numpy()\n","\n","X_temp1 = First.drop(First.columns.difference(['FI', 'C_SE1 > FI', 'C_FI > SE1', 'C_SE3 > FI', 'C_FI > SE3', 'C_EE > FI', 'C_FI > EE', 'F_FI > SE1', 'F_SE1 > FI', 'F_FI > SE3', 'F_SE3 > FI', 'F_FI > EE', 'F_EE > FI', 'FI Buy', 'FI Sell']), axis=1)\n","X_temp2 = First.drop(['FI', 'C_SE1 > FI', 'C_FI > SE1', 'C_SE3 > FI', 'C_FI > SE3', 'C_EE > FI', 'C_FI > EE', 'F_FI > SE1', 'F_SE1 > FI', 'F_FI > SE3', 'F_SE3 > FI', 'F_FI > EE', 'F_EE > FI', 'FI Buy', 'FI Sell'], axis=1)\n","X_temp2_index = X_temp2.index + DateOffset(hours=-hour_ahead)\n","X_temp2.set_index(X_temp2_index, inplace=True)\n","#X_temp1 = X_temp1.to_frame()\n","X_temp = X_temp1.join(X_temp2)\n","X = X_temp.iloc[0:N_index[0] - hour_ahead]\n","X = X.to_numpy()\n","Pred_index = X.shape\n","print(\"Shape = \"+str(Pred_index))\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n","\n","n_estimators = 80\n","learning_rate = 0.1\n","Depth = 6\n","Min_leaf = 20\n","\n","\n","class DecisionTree():\n","    def __init__(self, x, y, n_features, f_idxs, idxs=None, depth=3, min_leaf=5):\n","        if idxs is None: idxs=np.arange(len(y))\n","        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n","        self.depth = depth\n","        #print(f_idxs)\n","        #         print(self.depth)\n","        self.n_features = n_features\n","        self.n, self.c = len(idxs), x.shape[1]\n","        self.val = np.mean(y[idxs])\n","        self.score = float('inf')\n","        self.find_varsplit()\n","\n","    def find_varsplit(self):\n","        for i in self.f_idxs: self.find_better_split(i)\n","        if self.is_leaf: return\n","        x = self.split_col\n","        lhs = np.nonzero(x <= self.split)[0]\n","        rhs = np.nonzero(x > self.split)[0]\n","        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n","        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n","        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth - 1,\n","                                min_leaf=self.min_leaf)\n","        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth - 1,\n","                                min_leaf=self.min_leaf)\n","\n","    def find_better_split(self, var_idx):\n","        x, y = self.x[self.idxs, var_idx], self.y[self.idxs]\n","        sort_idx = np.argsort(x)\n","        sort_y, sort_x = y[sort_idx], x[sort_idx]\n","        rhs_sum2: object\n","        rhs_cnt, rhs_sum, rhs_sum2 = self.n, sort_y.sum(), (sort_y ** 2).sum()\n","        lhs_cnt, lhs_sum, lhs_sum2 = 0, 0., 0.\n","\n","        for i in range(0, self.n - self.min_leaf - 1):\n","            xi, yi = sort_x[i], sort_y[i]\n","            lhs_cnt += 1\n","            rhs_cnt -= 1\n","            lhs_sum += yi\n","            rhs_sum -= yi\n","            lhs_sum2 += yi ** 2\n","            rhs_sum2 -= yi ** 2\n","            if i < self.min_leaf or xi == sort_x[i + 1]:\n","                continue\n","\n","            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n","            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n","            curr_score = lhs_std * lhs_cnt + rhs_std * rhs_cnt\n","            if curr_score < self.score:\n","                self.var_idx, self.score, self.split = var_idx, curr_score, xi\n","\n","    @property\n","    def split_name(self):\n","        return self.x.columns[self.var_idx]\n","\n","    @property\n","    def split_col(self):\n","        return self.x[self.idxs, self.var_idx]\n","\n","    @property\n","    def is_leaf(self):\n","        return self.score == float('inf') or self.depth <= 0\n","\n","    def predict(self, x):\n","        return np.array([self.predict_row(xi) for xi in x])\n","\n","    def predict_row(self, xi):\n","        if self.is_leaf: return self.val\n","        t = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n","        return t.predict_row(xi)\n","\n","def std_agg(cnt, s1, s2):\n","    try:\n","        return math.sqrt((s2 / cnt) - (s1 / cnt) ** 2)\n","    except:\n","        return 0\n","\n","xi = X_train\n","train_index = X_train.shape\n","yi = y_train - np.mean(y_train)\n","ei = 0  # initialization of error\n","predf = np.mean(y_train)  # initial prediction 0\n","\n","\n","def create_tree(x, y, n_features, sample_sz, depth=3, min_leaf=5):\n","    idxs = np.random.permutation(len(y))[:sample_sz]\n","    f_idxs = np.random.permutation(x.shape[1])[:n_features]\n","    return DecisionTree(x[idxs], y[idxs], n_features, f_idxs,\n","                        idxs=np.array(range(sample_sz)), depth=depth, min_leaf=min_leaf)\n","\n","\n","print(\"hour_ahead = \" + str(hour_ahead))\n","print(\"n_estimators = \" + str(n_estimators))\n","print(\"learning_rate = \" + str(learning_rate))\n","print(\"Depth = \" + str(Depth))\n","print(\"Min_leaf = \" + str(Min_leaf))\n","\n","\n","trees = []\n","for i in range(n_estimators):\n","    tree = create_tree(xi, yi, train_index[1], train_index[0], depth=Depth, min_leaf=Min_leaf)\n","    predi = tree.predict(xi)\n","    predf = predf + learning_rate * predi\n","    print(str(i)+str(\"  \")+str(mean_absolute_error(y_train, predf)))\n","    ei = y_train - predf  # needed originl y here as residual always from original y\n","    yi = ei  # update yi as residual to reloop\n","    trees.append(tree)\n","\n","\n","MSE = mean_squared_error(y_train, predf, squared=True)\n","print(\"Training MSE  \" + str(MSE))\n","RMSE = np.sqrt(MSE)\n","print(\"Training RMSE  \" + str(RMSE))\n","MAE = mean_absolute_error(y_train, predf)\n","print(\"Training MAE  \" + str(MAE))\n","\n","\n","\n","xi = X_test\n","train_index = X_test.shape\n","yi = y_test - np.mean(y_test)\n","ei = 0  # initialization of error\n","predf = np.mean(y_test)\n","\n","i = 0\n","for t in trees:\n","    predi = t.predict(X_test)\n","    predf = predf + learning_rate * predi\n","    print(str(i)+str(\"  \")+str(mean_absolute_error(y_test, predf)))\n","    ei = y_test - predf  # needed originl y here as residual always from original y\n","    yi = ei  # update yi as residual to reloop\n","\n","    #if i == 6:\n","    #   break\n","    i += 1\n","\n","\n","MSE = mean_squared_error(y_test, predf, squared=True)\n","print(\"Testing MSE  \" + str(MSE))\n","RMSE = np.sqrt(MSE)\n","print(\"Testing RMSE  \" + str(RMSE))\n","MAE = mean_absolute_error(y_test, predf)\n","print(\"Testing MAE  \" + str(MAE))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Shape = (10976, 15)\n","hour_ahead = 24\n","n_estimators = 80\n","learning_rate = 0.1\n","Depth = 6\n","Min_leaf = 20\n","0  7.194888833978535\n","1  6.8523937586374055\n","2  6.532834832084875\n","3  6.274108388229512\n","4  6.054594588066795\n","5  5.867453572460393\n","6  5.689879352363645\n","7  5.55017746578964\n","8  5.426819068249473\n","9  5.3113110756085415\n","10  5.208491527752026\n","11  5.12466553428469\n","12  5.04927134373944\n","13  4.983181314532257\n","14  4.930191914721563\n","15  4.87470191646885\n","16  4.824015316962909\n","17  4.784636169098183\n","18  4.742725712651093\n","19  4.716894765778808\n","20  4.693911903491205\n","21  4.677419019160144\n","22  4.6622474095314255\n","23  4.648794423309669\n","24  4.638994233814678\n","25  4.631901148786128\n","26  4.623951530408011\n","27  4.614742447997617\n","28  4.606833711709861\n","29  4.601470808668361\n","30  4.594529853239584\n","31  4.591203158101483\n","32  4.587857082483199\n","33  4.583948108854353\n","34  4.579789404300489\n","35  4.5767190355933325\n","36  4.575486305248439\n","37  4.5741039661553815\n","38  4.5726237533946685\n","39  4.571211658170537\n","40  4.570383438638866\n","41  4.5694788992702735\n","42  4.568911777408028\n","43  4.568411469704815\n","44  4.567065561587672\n","45  4.566018843683056\n","46  4.5658009769225165\n","47  4.56404901790402\n","48  4.5635261639470786\n","49  4.563307633620722\n","50  4.563131368720113\n","51  4.562937118808668\n","52  4.560323066427514\n","53  4.559339940785085\n","54  4.5587860418748205\n","55  4.557876476145879\n","56  4.557830330926118\n","57  4.556965616621673\n","58  4.556463004403497\n","59  4.555664268112673\n","60  4.554899093942238\n","61  4.554098581991868\n","62  4.553736282562652\n","63  4.553340604597793\n","64  4.552331559364814\n","65  4.549951217354799\n","66  4.549744981529147\n","67  4.549406183621933\n","68  4.548592166673578\n","69  4.547084489756817\n","70  4.546855773334457\n","71  4.545100619883051\n","72  4.545075777644094\n","73  4.544502812314883\n","74  4.544335046393915\n","75  4.544345841304276\n","76  4.544362013911969\n","77  4.544352721957997\n","78  4.544363663617161\n","79  4.54428766233219\n","Training MSE  57.81043455509193\n","Training RMSE  7.603317338839142\n","Training MAE  4.54428766233219\n","0  7.780709452201014\n","1  7.630721386928212\n","2  7.510974358629497\n","3  7.429341925071921\n","4  7.349623471444278\n","5  7.335808124539536\n","6  7.328958127783239\n","7  7.331689150068877\n","8  7.333499467965496\n","9  7.35189968851807\n","10  7.383683897096454\n","11  7.426828720449824\n","12  7.472190396642694\n","13  7.518496193486939\n","14  7.557512324069607\n","15  7.6554293610173065\n","16  7.709462142142772\n","17  7.766047800909071\n","18  7.822294488937439\n","19  7.852663536454372\n","20  7.879703300860073\n","21  7.912308244493238\n","22  7.933595236155098\n","23  7.9584976036030834\n","24  7.984457225023715\n","25  7.997364517373209\n","26  8.012204366825953\n","27  8.026113686393645\n","28  8.039723764264313\n","29  8.052215205235376\n","30  8.085527132267595\n","31  8.098579928024794\n","32  8.110510997728765\n","33  8.132637579809256\n","34  8.147218643182383\n","35  8.156683955036263\n","36  8.164443092031913\n","37  8.171204772773308\n","38  8.189241195110437\n","39  8.201697587946649\n","40  8.205433369468237\n","41  8.216494059550271\n","42  8.22645055068545\n","43  8.227889512298988\n","44  8.236958644729718\n","45  8.237693521760827\n","46  8.245416231266708\n","47  8.2520203267835\n","48  8.25791576879554\n","49  8.256686740422868\n","50  8.26159680392544\n","51  8.260175431299512\n","52  8.267847323083865\n","53  8.26619318322649\n","54  8.264704457354853\n","55  8.263290017538198\n","56  8.262087468177878\n","57  8.261081509029436\n","58  8.265109505214934\n","59  8.268733997361878\n","60  8.271642841601238\n","61  8.27390669943281\n","62  8.27627770976674\n","63  8.278401168168797\n","64  8.279965362915034\n","65  8.303149974367614\n","66  8.305138092116513\n","67  8.306905375767586\n","68  8.308495230919036\n","69  8.310077668689214\n","70  8.311409090629729\n","71  8.307647065542545\n","72  8.309316271365986\n","73  8.310819254635435\n","74  8.311848969672479\n","75  8.313113245789031\n","76  8.314253900338251\n","77  8.315279793806619\n","78  8.316203097928147\n","79  8.317009108819654\n","Testing MSE  157.06717284350148\n","Testing RMSE  12.532644287759128\n","Testing MAE  8.317009108819654\n"],"name":"stdout"}]}]}