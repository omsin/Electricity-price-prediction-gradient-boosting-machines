{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Only_TimeFeature_Testing.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1HyJwRp4FDQ4MonLTf27o47DQ3YvGn52R","authorship_tag":"ABX9TyN9r64aZllBmrKRrIV/KKP2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lH6o_dL2_UHA","executionInfo":{"status":"ok","timestamp":1628231639036,"user_tz":-60,"elapsed":117385,"user":{"displayName":"Pasinpat Vitoochuleechoti","photoUrl":"","userId":"04423538674438574487"}},"outputId":"852a6218-517b-45b2-97fb-e1b915cc07ce"},"source":["import pandas as pd\n","import numpy as np\n","from IPython.display import display\n","from fastai.imports import *\n","from sklearn import metrics\n","from pandas.tseries.offsets import DateOffset\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","\n","\n","\n","hour_ahead = 24\n","path = \"/content/drive/MyDrive/Colab Notebooks/Project/Time_FI.csv\"\n","Merge_all = pd.read_csv(path)\n","dti = pd.to_datetime(Merge_all['DateTime'], format='%m/%d/%Y %H', exact=False)\n","Merge_all.set_index(pd.Index(dti), inplace=True)\n","Merge_all.drop(['DateTime'], axis=1, inplace=True)\n","First = Merge_all.iloc[0:11000, :]\n","N_index = First.shape\n","Y_temp = First['FI']\n","Y = Y_temp.iloc[hour_ahead:N_index[0]]\n","Y = Y.to_numpy()\n","\n","X_temp1 = First['FI']\n","X_temp2 = First.loc[:, First.columns != 'FI']\n","X_temp2_index = X_temp2.index + DateOffset(hours=-hour_ahead)\n","X_temp2.set_index(X_temp2_index, inplace=True)\n","X_temp1 = X_temp1.to_frame()\n","X_temp = X_temp1.join(X_temp2)\n","X = X_temp.iloc[0:N_index[0] - hour_ahead]\n","X = X.to_numpy()\n","Pred_index = X.shape\n","print(\"Shape = \"+str(Pred_index))\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n","\n","n_estimators = 80\n","learning_rate = 0.1\n","Depth = 6\n","Min_leaf = 20\n","\n","\n","class DecisionTree():\n","    def __init__(self, x, y, n_features, f_idxs, idxs=None, depth=3, min_leaf=5):\n","        if idxs is None: idxs=np.arange(len(y))\n","        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n","        self.depth = depth\n","        #print(f_idxs)\n","        #         print(self.depth)\n","        self.n_features = n_features\n","        self.n, self.c = len(idxs), x.shape[1]\n","        self.val = np.mean(y[idxs])\n","        self.score = float('inf')\n","        self.find_varsplit()\n","\n","    def find_varsplit(self):\n","        for i in self.f_idxs: self.find_better_split(i)\n","        if self.is_leaf: return\n","        x = self.split_col\n","        lhs = np.nonzero(x <= self.split)[0]\n","        rhs = np.nonzero(x > self.split)[0]\n","        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n","        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n","        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth - 1,\n","                                min_leaf=self.min_leaf)\n","        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth - 1,\n","                                min_leaf=self.min_leaf)\n","\n","    def find_better_split(self, var_idx):\n","        x, y = self.x[self.idxs, var_idx], self.y[self.idxs]\n","        sort_idx = np.argsort(x)\n","        sort_y, sort_x = y[sort_idx], x[sort_idx]\n","        rhs_sum2: object\n","        rhs_cnt, rhs_sum, rhs_sum2 = self.n, sort_y.sum(), (sort_y ** 2).sum()\n","        lhs_cnt, lhs_sum, lhs_sum2 = 0, 0., 0.\n","\n","        for i in range(0, self.n - self.min_leaf - 1):\n","            xi, yi = sort_x[i], sort_y[i]\n","            lhs_cnt += 1\n","            rhs_cnt -= 1\n","            lhs_sum += yi\n","            rhs_sum -= yi\n","            lhs_sum2 += yi ** 2\n","            rhs_sum2 -= yi ** 2\n","            if i < self.min_leaf or xi == sort_x[i + 1]:\n","                continue\n","\n","            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n","            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n","            curr_score = lhs_std * lhs_cnt + rhs_std * rhs_cnt\n","            if curr_score < self.score:\n","                self.var_idx, self.score, self.split = var_idx, curr_score, xi\n","\n","    @property\n","    def split_name(self):\n","        return self.x.columns[self.var_idx]\n","\n","    @property\n","    def split_col(self):\n","        return self.x[self.idxs, self.var_idx]\n","\n","    @property\n","    def is_leaf(self):\n","        return self.score == float('inf') or self.depth <= 0\n","\n","    def predict(self, x):\n","        return np.array([self.predict_row(xi) for xi in x])\n","\n","    def predict_row(self, xi):\n","        if self.is_leaf: return self.val\n","        t = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n","        return t.predict_row(xi)\n","\n","def std_agg(cnt, s1, s2):\n","    try:\n","        return math.sqrt((s2 / cnt) - (s1 / cnt) ** 2)\n","    except:\n","        return 0\n","\n","xi = X_train\n","train_index = X_train.shape\n","yi = y_train - np.mean(y_train)\n","ei = 0  # initialization of error\n","predf = np.mean(y_train)  # initial prediction 0\n","\n","\n","def create_tree(x, y, n_features, sample_sz, depth=3, min_leaf=5):\n","    idxs = np.random.permutation(len(y))[:sample_sz]\n","    f_idxs = np.random.permutation(x.shape[1])[:n_features]\n","    return DecisionTree(x[idxs], y[idxs], n_features, f_idxs,\n","                        idxs=np.array(range(sample_sz)), depth=depth, min_leaf=min_leaf)\n","\n","\n","print(\"hour_ahead = \" + str(hour_ahead))\n","print(\"n_estimators = \" + str(n_estimators))\n","print(\"learning_rate = \" + str(learning_rate))\n","print(\"Depth = \" + str(Depth))\n","print(\"Min_leaf = \" + str(Min_leaf))\n","\n","\n","trees = []\n","for i in range(n_estimators):\n","    tree = create_tree(xi, yi, train_index[1], train_index[0], depth=Depth, min_leaf=Min_leaf)\n","    predi = tree.predict(xi)\n","    predf = predf + learning_rate * predi\n","    print(str(i)+str(\"  \")+str(mean_absolute_error(y_train, predf)))\n","    ei = y_train - predf  # needed originl y here as residual always from original y\n","    yi = ei  # update yi as residual to reloop\n","    trees.append(tree)\n","\n","\n","MSE = mean_squared_error(y_train, predf, squared=True)\n","print(\"Training MSE  \" + str(MSE))\n","RMSE = np.sqrt(MSE)\n","print(\"Training RMSE  \" + str(RMSE))\n","MAE = mean_absolute_error(y_train, predf)\n","print(\"Training MAE  \" + str(MAE))\n","\n","\n","\n","xi = X_test\n","train_index = X_test.shape\n","yi = y_test - np.mean(y_test)\n","ei = 0  # initialization of error\n","predf = np.mean(y_test)\n","\n","i = 0\n","for t in trees:\n","    predi = t.predict(X_test)\n","    predf = predf + learning_rate * predi\n","    print(str(i)+str(\"  \")+str(mean_absolute_error(y_test, predf)))\n","    ei = y_test - predf  # needed originl y here as residual always from original y\n","    yi = ei  # update yi as residual to reloop\n","\n","    #if i == 6:\n","    #   break\n","    i += 1\n","\n","\n","MSE = mean_squared_error(y_test, predf, squared=True)\n","print(\"Testing MSE  \" + str(MSE))\n","RMSE = np.sqrt(MSE)\n","print(\"Testing RMSE  \" + str(RMSE))\n","MAE = mean_absolute_error(y_test, predf)\n","print(\"Testing MAE  \" + str(MAE))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Shape = (10976, 10)\n","hour_ahead = 24\n","n_estimators = 80\n","learning_rate = 0.1\n","Depth = 6\n","Min_leaf = 20\n","0  7.011903252822456\n","1  6.502868422264298\n","2  6.065702075906954\n","3  5.69187722340121\n","4  5.368456910965042\n","5  5.097480293335352\n","6  4.866287657342157\n","7  4.6719132741129386\n","8  4.511535560262199\n","9  4.365462272957584\n","10  4.243941803857308\n","11  4.140961159905265\n","12  4.056577005480964\n","13  3.9906157664149378\n","14  3.936492583861251\n","15  3.8864338446898516\n","16  3.845744680943257\n","17  3.811319663768125\n","18  3.7855212383948778\n","19  3.7626690843090724\n","20  3.745627425353645\n","21  3.731501339337487\n","22  3.7169125531347276\n","23  3.7042121568583424\n","24  3.696736975070427\n","25  3.689674783551206\n","26  3.681443711144575\n","27  3.6777963713919086\n","28  3.671021827782372\n","29  3.665676686758914\n","30  3.66222432378598\n","31  3.660727567175773\n","32  3.659800781873419\n","33  3.658119299268805\n","34  3.6563543870890087\n","35  3.6555685447709068\n","36  3.6549636633649274\n","37  3.6533021250222895\n","38  3.6521780306853664\n","39  3.6509955214325327\n","40  3.6508845533976304\n","41  3.6505899071570513\n","42  3.6504399179353193\n","43  3.650650759675995\n","44  3.650124040551299\n","45  3.6501708451188972\n","46  3.6502811953189522\n","47  3.6497443049642593\n","48  3.6498560794428987\n","49  3.6496902148559944\n","50  3.649365808234696\n","51  3.649427096678828\n","52  3.6492203582423635\n","53  3.6485757078018874\n","54  3.6484111903366983\n","55  3.6483650228569493\n","56  3.6483434157427728\n","57  3.6462629774422144\n","58  3.6463086174450856\n","59  3.6464442339973915\n","60  3.6462178717227567\n","61  3.6462597336473546\n","62  3.646356046913417\n","63  3.6463962825243947\n","64  3.6464005708668306\n","65  3.6464212397023754\n","66  3.64653535461343\n","67  3.6465274029379406\n","68  3.646494279217323\n","69  3.6465733581793134\n","70  3.646591482451262\n","71  3.6466139185172897\n","72  3.6466186637060165\n","73  3.6466559671941416\n","74  3.646670591186401\n","75  3.6467310050569983\n","76  3.6467345770955384\n","77  3.646326613274934\n","78  3.6463235404395036\n","79  3.646369785273481\n","Training MSE  45.17292183453287\n","Training RMSE  6.721080406789735\n","Training MAE  3.646369785273481\n","0  7.384745952005142\n","1  6.8841240182050765\n","2  6.40235336789259\n","3  5.99258303808353\n","4  5.668850001334083\n","5  5.5200084426692655\n","6  5.42324071498476\n","7  5.390717705461234\n","8  5.396713473355489\n","9  5.394442355762665\n","10  5.422766820401697\n","11  5.45422257657285\n","12  5.520810514885498\n","13  5.542089416185814\n","14  5.604666802565854\n","15  5.622273125924137\n","16  5.662098982752223\n","17  5.678094824337964\n","18  5.707711435991782\n","19  5.720836143940291\n","20  5.744869151463754\n","21  5.768793508006366\n","22  5.770618823729735\n","23  5.769222700786316\n","24  5.841205108218667\n","25  5.84427858316047\n","26  5.842633605022355\n","27  5.908235866172613\n","28  5.9668214036148095\n","29  5.965207932349165\n","30  6.021851070565873\n","31  6.031595403697965\n","32  6.083714407595722\n","33  6.085320593060525\n","34  6.074893975349832\n","35  6.123302905827016\n","36  6.16607429066278\n","37  6.206804332629262\n","38  6.204559617237558\n","39  6.2159829159762685\n","40  6.252420362628904\n","41  6.285910135692896\n","42  6.297698952804037\n","43  6.327747044671891\n","44  6.338410526612108\n","45  6.348360360487811\n","46  6.3761308159610035\n","47  6.368661389600312\n","48  6.378439592784259\n","49  6.389375318417626\n","50  6.414186752670304\n","51  6.421690595610127\n","52  6.4437899633487135\n","53  6.432920073745712\n","54  6.441856712132509\n","55  6.447439245782909\n","56  6.4523014891765\n","57  6.456977393016223\n","58  6.480846467530278\n","59  6.486409251711484\n","60  6.507786610906694\n","61  6.511073693452465\n","62  6.514095026475305\n","63  6.516756056087553\n","64  6.5192119851997425\n","65  6.521493643695244\n","66  6.524974296909104\n","67  6.544148667043312\n","68  6.561426960073658\n","69  6.563007079095035\n","70  6.564371267513098\n","71  6.565599037089356\n","72  6.581501848945756\n","73  6.58249706319735\n","74  6.583401140611134\n","75  6.584276871586425\n","76  6.585010138251604\n","77  6.587285169917266\n","78  6.587867429528173\n","79  6.588449024595078\n","Testing MSE  77.35363359267743\n","Testing RMSE  8.795091448795597\n","Testing MAE  6.588449024595078\n"],"name":"stdout"}]}]}